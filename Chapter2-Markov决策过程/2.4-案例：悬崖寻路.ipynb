{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import scipy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "观测空间 = Discrete(48)\n",
      "动作空间 = Discrete(4)\n",
      "观测数量 = 48, 动作数量 = 4\n",
      "地图大小 = (4, 12)\n"
     ]
    }
   ],
   "source": [
    "# 导入‘CliffWalking-v0’环境\n",
    "\n",
    "env = gym.make('CliffWalking-v0')\n",
    "print('观测空间 = {}'.format(env.observation_space))\n",
    "print('动作空间 = {}'.format(env.action_space))\n",
    "print('观测数量 = {}, 动作数量 = {}'.format(env.nS, env.nA))\n",
    "print('地图大小 = {}'.format(env.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行一个回合\n",
    "\n",
    "def play_once(env, policy):\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        loc = np.unravel_index(state, env.shape)\n",
    "        print('状态 = {}, 位置 = {}'.format(state, loc), end = ' | ')\n",
    "        action = np.random.choice(env.nA, p=policy[state])\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        print('动作 = {}, 奖励 = {}'.format(action, reward))\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最优策略 = \n",
      "[[1 1 1 1 1 1 1 1 1 1 1 2]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 2]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 2]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 2]]\n",
      "\n",
      "状态 = 36, 位置 = (3, 0) | 动作 = 0, 奖励 = -1\n",
      "状态 = 24, 位置 = (2, 0) | 动作 = 1, 奖励 = -1\n",
      "状态 = 25, 位置 = (2, 1) | 动作 = 1, 奖励 = -1\n",
      "状态 = 26, 位置 = (2, 2) | 动作 = 1, 奖励 = -1\n",
      "状态 = 27, 位置 = (2, 3) | 动作 = 1, 奖励 = -1\n",
      "状态 = 28, 位置 = (2, 4) | 动作 = 1, 奖励 = -1\n",
      "状态 = 29, 位置 = (2, 5) | 动作 = 1, 奖励 = -1\n",
      "状态 = 30, 位置 = (2, 6) | 动作 = 1, 奖励 = -1\n",
      "状态 = 31, 位置 = (2, 7) | 动作 = 1, 奖励 = -1\n",
      "状态 = 32, 位置 = (2, 8) | 动作 = 1, 奖励 = -1\n",
      "状态 = 33, 位置 = (2, 9) | 动作 = 1, 奖励 = -1\n",
      "状态 = 34, 位置 = (2, 10) | 动作 = 1, 奖励 = -1\n",
      "状态 = 35, 位置 = (2, 11) | 动作 = 2, 奖励 = -1\n",
      "总奖励 = -13\n"
     ]
    }
   ],
   "source": [
    "# 最优策略\n",
    "\n",
    "actions = np.ones(env.shape, dtype=int)\n",
    "actions[-1, :] = 0\n",
    "actions[:, -1] = 2\n",
    "optimal_policy = np.eye(4)[actions.reshape(-1)]\n",
    "print('最优策略 = \\n{}\\n'.format(actions))\n",
    "\n",
    "total_reward = play_once(env, optimal_policy)\n",
    "print('总奖励 = {}'.format(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用Bellman方程求解状态价值和动作价值\n",
    "\n",
    "def evaluate_bellman(env, policy, gamma=1.0):\n",
    "    a, b = np.eye(env.nS), np.zeros((env.nS))\n",
    "    for state in range(env.nS - 1):\n",
    "        for action in range(env.nA):\n",
    "            pi = policy[state][action]\n",
    "            for p, next_state, reward, done in env.P[state][action]:\n",
    "                a[state, next_state] -= pi * gamma * p\n",
    "                b[state] += pi * reward * p\n",
    "    v = np.linalg.solve(a, b)\n",
    "    \n",
    "    q = np.zeros((env.nS, env.nA))\n",
    "    for state in range(env.nS - 1):\n",
    "        for action in range(env.nA):\n",
    "            for p, next_state, reward, done in env.P[state][action]:\n",
    "                q[state][action] += (reward + gamma * v[next_state]) * p\n",
    "    return v, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态价值 = [-84537767.17599674 -84537734.14924777 -84537128.96237774\n",
      " -84536314.46853566 -84534411.5386176  -84523673.65553102\n",
      " -84484789.05526869 -84373265.75787517 -84329922.07866065\n",
      " -84275296.29400891 -84157473.9489998  -84126181.5476279\n",
      " -84537790.11340794 -84537698.61188367 -84537527.58746669\n",
      " -84537068.71889049 -84535609.97717507 -84519784.83687702\n",
      " -84501674.3798621  -84427458.1592079  -84303436.46428464\n",
      " -84229578.6346756  -84101343.11962558 -84084223.0419421\n",
      " -84538076.4686468  -84538041.2098556  -84537974.47486386\n",
      " -84537748.23394884 -84536900.56879608 -84526997.16611306\n",
      " -84524811.57365067 -84514151.94362962 -84382801.25759234\n",
      " -84220081.03580694 -83779780.71472944 -82915912.33177578\n",
      " -84538305.52849488 -84538312.12583822 -84538153.59525718\n",
      " -84538145.98539561 -84538403.63886732 -84535498.19949386\n",
      " -84533771.18294747 -84535051.68012775 -84496447.7055068\n",
      " -84499725.97483775 -63199813.39233176         0.        ]\n",
      "动作价值 = [[-8.45377682e+07 -8.45377351e+07 -8.45377911e+07 -8.45377682e+07]\n",
      " [-8.45377351e+07 -8.45371300e+07 -8.45376996e+07 -8.45377682e+07]\n",
      " [-8.45371300e+07 -8.45363155e+07 -8.45375286e+07 -8.45377351e+07]\n",
      " [-8.45363155e+07 -8.45344125e+07 -8.45370697e+07 -8.45371300e+07]\n",
      " [-8.45344125e+07 -8.45236747e+07 -8.45356110e+07 -8.45363155e+07]\n",
      " [-8.45236747e+07 -8.44847901e+07 -8.45197858e+07 -8.45344125e+07]\n",
      " [-8.44847901e+07 -8.43732668e+07 -8.45016754e+07 -8.45236747e+07]\n",
      " [-8.43732668e+07 -8.43299231e+07 -8.44274592e+07 -8.44847901e+07]\n",
      " [-8.43299231e+07 -8.42752973e+07 -8.43034375e+07 -8.43732668e+07]\n",
      " [-8.42752973e+07 -8.41574749e+07 -8.42295796e+07 -8.43299231e+07]\n",
      " [-8.41574749e+07 -8.41261825e+07 -8.41013441e+07 -8.42752973e+07]\n",
      " [-8.41261825e+07 -8.41261825e+07 -8.40842240e+07 -8.41574749e+07]\n",
      " [-8.45377682e+07 -8.45376996e+07 -8.45380775e+07 -8.45377911e+07]\n",
      " [-8.45377351e+07 -8.45375286e+07 -8.45380422e+07 -8.45377911e+07]\n",
      " [-8.45371300e+07 -8.45370697e+07 -8.45379755e+07 -8.45376996e+07]\n",
      " [-8.45363155e+07 -8.45356110e+07 -8.45377492e+07 -8.45375286e+07]\n",
      " [-8.45344125e+07 -8.45197858e+07 -8.45369016e+07 -8.45370697e+07]\n",
      " [-8.45236747e+07 -8.45016754e+07 -8.45269982e+07 -8.45356110e+07]\n",
      " [-8.44847901e+07 -8.44274592e+07 -8.45248126e+07 -8.45197858e+07]\n",
      " [-8.43732668e+07 -8.43034375e+07 -8.45141529e+07 -8.45016754e+07]\n",
      " [-8.43299231e+07 -8.42295796e+07 -8.43828023e+07 -8.44274592e+07]\n",
      " [-8.42752973e+07 -8.41013441e+07 -8.42200820e+07 -8.43034375e+07]\n",
      " [-8.41574749e+07 -8.40842240e+07 -8.37797817e+07 -8.42295796e+07]\n",
      " [-8.41261825e+07 -8.40842240e+07 -8.29159133e+07 -8.41013441e+07]\n",
      " [-8.45377911e+07 -8.45380422e+07 -8.45383065e+07 -8.45380775e+07]\n",
      " [-8.45376996e+07 -8.45379755e+07 -8.45384055e+07 -8.45380775e+07]\n",
      " [-8.45375286e+07 -8.45377492e+07 -8.45384055e+07 -8.45380422e+07]\n",
      " [-8.45370697e+07 -8.45369016e+07 -8.45384055e+07 -8.45379755e+07]\n",
      " [-8.45356110e+07 -8.45269982e+07 -8.45384055e+07 -8.45377492e+07]\n",
      " [-8.45197858e+07 -8.45248126e+07 -8.45384055e+07 -8.45369016e+07]\n",
      " [-8.45016754e+07 -8.45141529e+07 -8.45384055e+07 -8.45269982e+07]\n",
      " [-8.44274592e+07 -8.43828023e+07 -8.45384055e+07 -8.45248126e+07]\n",
      " [-8.43034375e+07 -8.42200820e+07 -8.45384055e+07 -8.45141529e+07]\n",
      " [-8.42295796e+07 -8.37797817e+07 -8.45384055e+07 -8.43828023e+07]\n",
      " [-8.41013441e+07 -8.29159133e+07 -8.45384055e+07 -8.42200820e+07]\n",
      " [-8.40842240e+07 -8.29159133e+07 -1.00000000e+00 -8.37797817e+07]\n",
      " [-8.45380775e+07 -8.45384055e+07 -8.45383065e+07 -8.45383065e+07]\n",
      " [-8.45380422e+07 -8.45384055e+07 -8.45384055e+07 -8.45383065e+07]\n",
      " [-8.45379755e+07 -8.45384055e+07 -8.45384055e+07 -8.45384055e+07]\n",
      " [-8.45377492e+07 -8.45384055e+07 -8.45384055e+07 -8.45384055e+07]\n",
      " [-8.45369016e+07 -8.45384055e+07 -8.45384055e+07 -8.45384055e+07]\n",
      " [-8.45269982e+07 -8.45384055e+07 -8.45384055e+07 -8.45384055e+07]\n",
      " [-8.45248126e+07 -8.45384055e+07 -8.45384055e+07 -8.45384055e+07]\n",
      " [-8.45141529e+07 -8.45384055e+07 -8.45384055e+07 -8.45384055e+07]\n",
      " [-8.43828023e+07 -8.45384055e+07 -8.45384055e+07 -8.45384055e+07]\n",
      " [-8.42200820e+07 -8.45384055e+07 -8.45384055e+07 -8.45384055e+07]\n",
      " [-8.37797817e+07 -1.00000000e+00 -8.45384055e+07 -8.45384055e+07]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# 评估随机策略\n",
    "\n",
    "policy = np.random.uniform(size=(env.nS, env.nA))\n",
    "policy = policy / np.sum(policy, axis=1)[:, np.newaxis]\n",
    "state_values, action_values = evaluate_bellman(env, policy)\n",
    "print('状态价值 = {}'.format(state_values))\n",
    "print('动作价值 = {}'.format(action_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最优状态价值 = [-14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3. -13. -12.\n",
      " -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2. -12. -11. -10.  -9.\n",
      "  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1. -13. -12. -11. -10.  -9.  -8.\n",
      "  -7.  -6.  -5.  -4.  -3.   0.]\n",
      "最优动作价值 = [[ -15.  -14.  -14.  -15.]\n",
      " [ -14.  -13.  -13.  -15.]\n",
      " [ -13.  -12.  -12.  -14.]\n",
      " [ -12.  -11.  -11.  -13.]\n",
      " [ -11.  -10.  -10.  -12.]\n",
      " [ -10.   -9.   -9.  -11.]\n",
      " [  -9.   -8.   -8.  -10.]\n",
      " [  -8.   -7.   -7.   -9.]\n",
      " [  -7.   -6.   -6.   -8.]\n",
      " [  -6.   -5.   -5.   -7.]\n",
      " [  -5.   -4.   -4.   -6.]\n",
      " [  -4.   -4.   -3.   -5.]\n",
      " [ -15.  -13.  -13.  -14.]\n",
      " [ -14.  -12.  -12.  -14.]\n",
      " [ -13.  -11.  -11.  -13.]\n",
      " [ -12.  -10.  -10.  -12.]\n",
      " [ -11.   -9.   -9.  -11.]\n",
      " [ -10.   -8.   -8.  -10.]\n",
      " [  -9.   -7.   -7.   -9.]\n",
      " [  -8.   -6.   -6.   -8.]\n",
      " [  -7.   -5.   -5.   -7.]\n",
      " [  -6.   -4.   -4.   -6.]\n",
      " [  -5.   -3.   -3.   -5.]\n",
      " [  -4.   -3.   -2.   -4.]\n",
      " [ -14.  -12.  -14.  -13.]\n",
      " [ -13.  -11. -113.  -13.]\n",
      " [ -12.  -10. -113.  -12.]\n",
      " [ -11.   -9. -113.  -11.]\n",
      " [ -10.   -8. -113.  -10.]\n",
      " [  -9.   -7. -113.   -9.]\n",
      " [  -8.   -6. -113.   -8.]\n",
      " [  -7.   -5. -113.   -7.]\n",
      " [  -6.   -4. -113.   -6.]\n",
      " [  -5.   -3. -113.   -5.]\n",
      " [  -4.   -2. -113.   -4.]\n",
      " [  -3.   -2.   -1.   -3.]\n",
      " [ -13. -113.  -14.  -14.]\n",
      " [ -12. -113. -113.  -14.]\n",
      " [ -11. -113. -113. -113.]\n",
      " [ -10. -113. -113. -113.]\n",
      " [  -9. -113. -113. -113.]\n",
      " [  -8. -113. -113. -113.]\n",
      " [  -7. -113. -113. -113.]\n",
      " [  -6. -113. -113. -113.]\n",
      " [  -5. -113. -113. -113.]\n",
      " [  -4. -113. -113. -113.]\n",
      " [  -3.   -1. -113. -113.]\n",
      " [   0.    0.    0.    0.]]\n"
     ]
    }
   ],
   "source": [
    "# 评估最优策略\n",
    "\n",
    "optimal_state_values, optimal_action_values = evaluate_bellman(env, optimal_policy)\n",
    "print('最优状态价值 = {}'.format(optimal_state_values))\n",
    "print('最优动作价值 = {}'.format(optimal_action_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最优状态价值 = [-1.40000000e+01 -1.30000000e+01 -1.20000000e+01 -1.10000000e+01\n",
      " -1.00000000e+01 -9.00000000e+00 -8.00000000e+00 -7.00000000e+00\n",
      " -6.00000000e+00 -5.00000000e+00 -4.00000000e+00 -3.00000000e+00\n",
      " -1.30000000e+01 -1.20000000e+01 -1.10000000e+01 -1.00000000e+01\n",
      " -9.00000000e+00 -8.00000000e+00 -7.00000000e+00 -6.00000000e+00\n",
      " -5.00000000e+00 -4.00000000e+00 -3.00000000e+00 -2.00000000e+00\n",
      " -1.20000000e+01 -1.10000000e+01 -1.00000000e+01 -9.00000000e+00\n",
      " -8.00000000e+00 -7.00000000e+00 -6.00000000e+00 -5.00000000e+00\n",
      " -4.00000000e+00 -3.00000000e+00 -2.00000000e+00 -1.00000000e+00\n",
      " -1.30000000e+01 -1.20000000e+01 -1.10000000e+01 -1.00000000e+01\n",
      " -9.00000000e+00 -8.00000000e+00 -7.00000000e+00 -6.00000000e+00\n",
      " -5.00000000e+00 -4.00000000e+00 -9.99999999e-01  1.82270928e-11]\n",
      "最优动作价值 = [[ -14.99999999  -13.99999999  -13.99999999  -14.99999999]\n",
      " [ -13.99999999  -13.          -13.          -14.99999999]\n",
      " [ -13.          -12.          -12.          -13.99999999]\n",
      " [ -12.          -11.          -11.          -13.        ]\n",
      " [ -11.          -10.          -10.          -12.        ]\n",
      " [ -10.           -9.           -9.          -11.        ]\n",
      " [  -9.           -8.           -8.          -10.        ]\n",
      " [  -8.           -7.           -7.           -9.        ]\n",
      " [  -7.           -6.           -6.           -8.        ]\n",
      " [  -6.           -5.           -5.           -7.        ]\n",
      " [  -5.           -4.           -4.           -6.        ]\n",
      " [  -4.           -4.           -3.           -5.        ]\n",
      " [ -14.99999999  -13.          -13.          -13.99999999]\n",
      " [ -13.99999999  -12.          -12.          -13.99999999]\n",
      " [ -13.          -11.          -11.          -13.        ]\n",
      " [ -12.          -10.          -10.          -12.        ]\n",
      " [ -11.           -9.           -9.          -11.        ]\n",
      " [ -10.           -8.           -8.          -10.        ]\n",
      " [  -9.           -7.           -7.           -9.        ]\n",
      " [  -8.           -6.           -6.           -8.        ]\n",
      " [  -7.           -5.           -5.           -7.        ]\n",
      " [  -6.           -4.           -4.           -6.        ]\n",
      " [  -5.           -3.           -3.           -5.        ]\n",
      " [  -4.           -3.           -2.           -4.        ]\n",
      " [ -13.99999999  -12.          -14.          -13.        ]\n",
      " [ -13.          -11.         -113.          -13.        ]\n",
      " [ -12.          -10.         -113.          -12.        ]\n",
      " [ -11.           -9.         -113.          -11.        ]\n",
      " [ -10.           -8.         -113.          -10.        ]\n",
      " [  -9.           -7.         -113.           -9.        ]\n",
      " [  -8.           -6.         -113.           -8.        ]\n",
      " [  -7.           -5.         -113.           -7.        ]\n",
      " [  -6.           -4.         -113.           -6.        ]\n",
      " [  -5.           -3.         -113.           -5.        ]\n",
      " [  -4.           -2.         -113.           -4.        ]\n",
      " [  -3.           -2.           -1.           -3.        ]\n",
      " [ -13.         -113.          -14.          -14.        ]\n",
      " [ -12.         -113.         -113.          -14.        ]\n",
      " [ -11.         -113.         -113.         -113.        ]\n",
      " [ -10.         -113.         -113.         -113.        ]\n",
      " [  -9.         -113.         -113.         -113.        ]\n",
      " [  -8.         -113.         -113.         -113.        ]\n",
      " [  -7.         -113.         -113.         -113.        ]\n",
      " [  -6.         -113.         -113.         -113.        ]\n",
      " [  -5.         -113.         -113.         -113.        ]\n",
      " [  -4.         -113.         -113.         -113.        ]\n",
      " [  -3.           -1.         -113.         -113.        ]\n",
      " [   0.            0.            0.            0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# 用线性规划求解Bellman最优方程\n",
    "\n",
    "def optimal_bellman(env, gamma=1.0):\n",
    "    p = np.zeros((env.nS, env.nA, env.nS))\n",
    "    r = np.zeros((env.nS, env.nA))\n",
    "    for state in range(env.nS - 1):\n",
    "        for action in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[state][action]:\n",
    "                p[state, action, next_state] += prob\n",
    "                r[state, action] += reward * prob\n",
    "    \n",
    "    c = np.ones((env.nS))\n",
    "    a_ub = gamma * p.reshape(-1, env.nS) - np.repeat(np.eye(env.nS), env.nA, axis=0)\n",
    "    b_ub = -r.reshape(-1)\n",
    "    \n",
    "    bounds = [(None, None),] * env.nS\n",
    "    res = scipy.optimize.linprog(c, a_ub, b_ub, bounds=bounds, method='interior-point')\n",
    "    v = res.x\n",
    "    q = r + gamma * np.dot(p, v)\n",
    "    return v, q\n",
    "\n",
    "optimal_state_values, optimal_action_values = optimal_bellman(env)\n",
    "print('最优状态价值 = {}'.format(optimal_state_values))\n",
    "print('最优动作价值 = {}'.format(optimal_action_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最优策略 = \n",
      "[[2 1 1 1 1 1 1 1 1 1 1 2]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 2]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 2]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# 用最优动作价值确定最优确定性策略\n",
    "\n",
    "optimal_actions = optimal_action_values.argmax(axis=1)\n",
    "print('最优策略 = \\n{}'.format(optimal_actions.reshape(env.shape)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
